{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "torch.manual_seed(666)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "# pip install seaborn\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core import Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"Dataset.auto_read_files\" is deprecated after version 1.0.69. Please use \"Dataset.Tabular.from_delimited_files\" instead. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n",
      "DatasetDefinition class is deprecated after version 1.0.69. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n",
      "The constructor of Dataset is deprecated after version 1.0.69. Please use factory methods from \"Dataset.Tabular\" and \"Dataset.File\" to create dataset instances. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n",
      "\"Dataset.get\" is deprecated after version 1.0.69. Please use \"Dataset.get_by_name\" and \"Dataset.get_by_id\" to retrieve dataset. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13057\n"
     ]
    }
   ],
   "source": [
    "# get the configs for the Azure Workspace\n",
    "ws = Workspace.from_config()\n",
    "# initialize location of the data (blob)\n",
    "datastore_name = 'mlthesisdatablob'\n",
    "dataset_name='data.cleaned.raw'\n",
    "#load the data of english examples\n",
    "datastore = Datastore.get(ws, datastore_name)\n",
    "datapath = datastore.path('data_all_lang.csv')\n",
    "dataset = Dataset.auto_read_files(datapath)\n",
    "\n",
    "d = dataset.register(workspace=ws, name=dataset_name, exist_ok=True, update_if_exist=True)\n",
    "\n",
    "dataset = Dataset.get(ws,dataset_name)\n",
    "# create a dataframe\n",
    "df = dataset.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "\n",
    "#Alternatively: Load data from csv in your directory \n",
    "\n",
    "\n",
    "#df = pd.read_csv('data_all_lang.csv')\n",
    "\n",
    "#df.head()\n",
    "## Preprocessing \n",
    "\n",
    "# create a dataset for each language\n",
    "\n",
    "df_en = df[df.lang_final=='en']\n",
    "df_es = df[df.lang_final=='es']\n",
    "df_it = df[df.lang_final=='it']\n",
    "df_hi = df[df.lang_final=='hi']\n",
    "df_pt = df[df.lang_final=='pt']\n",
    "df_fr = df[df.lang_final=='fr']\n",
    "\n",
    "# reset the indicies\n",
    "\n",
    "df_en.reset_index(drop=True, inplace=True)\n",
    "df_es.reset_index(drop=True, inplace=True)\n",
    "df_it.reset_index(drop=True, inplace=True)\n",
    "df_hi.reset_index(drop=True, inplace=True)\n",
    "df_pt.reset_index(drop=True, inplace=True)\n",
    "df_fr.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split_df(df, test_size , val_size, random_state, stratify_column):\n",
    "    # create a intermediary df and the test set\n",
    "    val_s = val_size/(1-test_size)\n",
    "    df_int, df_test = train_test_split(df,\n",
    "                                       stratify=df[stratify_column],\n",
    "                                       test_size=test_size,\n",
    "                                       random_state=random_state)\n",
    "    df_train, df_val = train_test_split(df_int,\n",
    "                                       stratify=df_int[stratify_column],\n",
    "                                       test_size=test_size,\n",
    "                                       random_state=random_state)\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    df_val = df_val.reset_index(drop=True)\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "\n",
    "MODEL_TYPE = 'xlm-roberta-base'\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)\n",
    "\n",
    "#from transformers import XLMRobertaForSequenceClassification\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    MODEL_TYPE, \n",
    "    num_labels = 2, # The number of output labels. 2 for binary classification.\n",
    ")\n",
    "\n",
    "# Send the model to the device.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The code for the following cell was created using inspiration from : \n",
    "https://www.kaggle.com/vbookshelf/basics-of-bert-and-xlm-roberta-pytorch\"\"\"\n",
    "\n",
    "\n",
    "class CompDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df_data = df\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.df_data.loc[index, 'text_feat_clean']\n",
    "        \n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(text,\n",
    "                                             add_special_tokens = True,\n",
    "                                             max_length = 512,\n",
    "                                             truncation=True,\n",
    "                                             pad_to_max_length=True,\n",
    "                                             return_attention_mask=True,\n",
    "                                             return_tensors ='pt')\n",
    "        \n",
    "        # These are torch tensors already.\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "        \n",
    "        # Convert the target to a torch tensor\n",
    "        target = torch.tensor(self.df_data.loc[index, 'target'])\n",
    "\n",
    "        sample = (padded_token_list, att_mask, target)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The code for the following cell was created using inspiration from : \n",
    "https://www.kaggle.com/vbookshelf/basics-of-bert-and-xlm-roberta-pytorch\"\"\"\n",
    "\n",
    "def train(train_dataloader, optimizer, NUM_EPOCHS, model_name):    \n",
    "\n",
    "\n",
    "\n",
    "    # Set the seed.\n",
    "    seed_val = 101\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Store the average loss after each epoch so we can plot them.\n",
    "    loss_values = []\n",
    "\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch in range(0, NUM_EPOCHS):\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n",
    "\n",
    "\n",
    "        stacked_val_labels = []\n",
    "        targets_list = []\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        print('Training...')\n",
    "\n",
    "        # put the model into train mode\n",
    "        model.train()\n",
    "\n",
    "        # This turns gradient calculations on and off.\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "            train_status = 'Batch ' + str(i) + ' of ' + str(len(train_dataloader))\n",
    "\n",
    "            print(train_status, end='\\r')\n",
    "\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "            # Get the loss from the outputs tuple: (loss, logits)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Convert the loss from a torch tensor to a number.\n",
    "            # Calculate the total loss.\n",
    "            total_train_loss = total_train_loss + loss.item()\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # Use the optimizer to update the weights.\n",
    "            optimizer.step() \n",
    "\n",
    "\n",
    "\n",
    "        print('Train loss:' ,total_train_loss/len(train_dataloader))\n",
    "\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "\n",
    "        print('\\nValidation...')\n",
    "\n",
    "        # Put the model in evaluation mode.\n",
    "        model.eval()\n",
    "\n",
    "        # Turn off the gradient calculations.\n",
    "        # This tells the model not to compute or store gradients.\n",
    "        # This step saves memory and speeds up validation.\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_val_loss = 0\n",
    "\n",
    "\n",
    "        for j, batch in enumerate(val_dataloader):\n",
    "\n",
    "            val_status = 'Batch ' + str(j) + ' of ' + str(len(val_dataloader))\n",
    "\n",
    "            print(val_status, end='\\r')\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)      \n",
    "\n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "            # Get the loss from the outputs tuple: (loss, logits)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Convert the loss from a torch tensor to a number.\n",
    "            # Calculate the total loss.\n",
    "            total_val_loss = total_val_loss + loss.item()\n",
    "\n",
    "\n",
    "            # Get the preds\n",
    "            preds = outputs[1]\n",
    "\n",
    "\n",
    "            # Move preds to the CPU\n",
    "            val_preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            # Move the labels to the cpu\n",
    "            targets_np = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Append the labels to a numpy list\n",
    "            targets_list.extend(targets_np)\n",
    "\n",
    "            if j == 0:  # first batch\n",
    "                stacked_val_preds = val_preds\n",
    "\n",
    "            else:\n",
    "                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
    "\n",
    "\n",
    "        # Calculate the validation accuracy\n",
    "        y_true = targets_list\n",
    "        y_pred = np.argmax(stacked_val_preds, axis=1)\n",
    "\n",
    "        val_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "        print('Val loss:' ,total_val_loss/len(val_dataloader))\n",
    "        print('Val acc: ', val_acc)\n",
    "\n",
    "\n",
    "        # Save the Model\n",
    "        torch.save(model.state_dict(), model_name+'.pt')\n",
    "\n",
    "    return model, loss_values\n",
    "\n",
    "\n",
    "def evaluate(test_dataloader, model):    \n",
    "    targets_list = []\n",
    "    for j, batch in enumerate(test_dataloader):\n",
    "\n",
    "            inference_status = 'Batch ' + str(j+1) + ' of ' + str(len(test_dataloader))\n",
    "\n",
    "            print(inference_status, end='\\r')\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                    attention_mask=b_input_mask)\n",
    "\n",
    "\n",
    "            # Get the preds\n",
    "            preds = outputs[0]\n",
    "\n",
    "\n",
    "            # Move preds to the CPU\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            # Move the labels to the cpu\n",
    "            targets_np = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Append the labels to a numpy list\n",
    "            targets_list.extend(targets_np)\n",
    "\n",
    "            # Stack the predictions.\n",
    "\n",
    "            if j == 0:  # first batch\n",
    "                stacked_preds = preds\n",
    "\n",
    "            else:\n",
    "                stacked_preds = np.vstack((stacked_preds, preds))\n",
    "            \n",
    "    y_true = targets_list\n",
    "    y_pred = np.argmax(stacked_preds, axis=1)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "            \n",
    "\n",
    "def x_language_eval(dataloader, model, fig_name):\n",
    "    y_true, y_pred = evaluate(dataloader, model)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    #figure = sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, cmap='Blues')\n",
    "    #fig = figure.get_figure()\n",
    "    #fig.savefig(fig_name)\n",
    "    \n",
    "def ouput_creator(train_lang, model, d_dataloader):\n",
    "    \n",
    "    for i in d_dataloader:\n",
    "        if i != train_lang:\n",
    "            print('============='+train_lang +' - '+i+'=============')\n",
    "            x_language_eval(d_dataloader[i], model, train_lang+'_'+i+'_b.png')\n",
    "        else:\n",
    "            print('============='+train_lang +' - '+i+'=============')\n",
    "            x_language_eval(test_dataloader, model, i+'_'+i+'_b.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_RATE = 2e-5\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "NUM_CORES = 6\n",
    "\n",
    "DF = df_fr\n",
    "\n",
    "MODEL_NAME = 'model_fr_unbalanced'\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "              lr = L_RATE, \n",
    "              eps = 1e-8 \n",
    "            )\n",
    "\n",
    "\n",
    "df_train, df_val, df_test = train_val_test_split_df(DF, 0.1,0.2,123, 'target')\n",
    "\n",
    "train_data = CompDataset(df_train)\n",
    "val_data = CompDataset(df_val)\n",
    "test_data = CompDataset(df_test)\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        sampler = RandomSampler(train_data),\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        sampler = SequentialSampler(val_data),\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              sampler=SequentialSampler(test_data), \n",
    "                                              num_workers=NUM_CORES)\n",
    "\n",
    "\n",
    "english_dataloader = torch.utils.data.DataLoader(CompDataset(df_en),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "\n",
    "\n",
    "spanish_dataloader = torch.utils.data.DataLoader(CompDataset(df_es),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "italian_dataloader = torch.utils.data.DataLoader(CompDataset(df_it),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "hindi_dataloader = torch.utils.data.DataLoader(CompDataset(df_hi),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "portuguese_dataloader = torch.utils.data.DataLoader(CompDataset(df_pt),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "french_dataloader = torch.utils.data.DataLoader(CompDataset(df_fr),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "d_dataloader = {'en':english_dataloader,\n",
    "                'es':spanish_dataloader,\n",
    "                'it':italian_dataloader,\n",
    "                'hi':hindi_dataloader,\n",
    "                'pt':portuguese_dataloader, \n",
    "                'fr':french_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "Train loss: 0.34404488173785847\n",
      "\n",
      "Validation...\n",
      "Val loss: 0.2360971526359208\n",
      "Val acc:  0.8870967741935484\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "Train loss: 0.20151745975591187\n",
      "\n",
      "Validation...\n",
      "Val loss: 0.07112256623440771\n",
      "Val acc:  0.9516129032258065\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "Train loss: 0.13682901209550977\n",
      "\n",
      "Validation...\n",
      "Val loss: 0.30701520765796886\n",
      "Val acc:  0.9516129032258065\n"
     ]
    }
   ],
   "source": [
    "model, loss_stats = train(train_dataloader, optimizer, NUM_EPOCHS, MODEL_NAME)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============en - en=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       462\n",
      "           1       0.96      0.92      0.94       330\n",
      "\n",
      "    accuracy                           0.95       792\n",
      "   macro avg       0.95      0.95      0.95       792\n",
      "weighted avg       0.95      0.95      0.95       792\n",
      "\n",
      "[[449  13]\n",
      " [ 26 304]]\n",
      "=============en - es=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.83      0.90      1291\n",
      "           1       0.40      0.86      0.55       168\n",
      "\n",
      "    accuracy                           0.84      1459\n",
      "   macro avg       0.69      0.85      0.73      1459\n",
      "weighted avg       0.91      0.84      0.86      1459\n",
      "\n",
      "[[1077  214]\n",
      " [  23  145]]\n",
      "=============en - it=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.72      0.83      1076\n",
      "           1       0.11      0.88      0.19        42\n",
      "\n",
      "    accuracy                           0.72      1118\n",
      "   macro avg       0.55      0.80      0.51      1118\n",
      "weighted avg       0.96      0.72      0.81      1118\n",
      "\n",
      "[[770 306]\n",
      " [  5  37]]\n",
      "=============en - hi=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       730\n",
      "           1       0.68      0.99      0.80        75\n",
      "\n",
      "    accuracy                           0.96       805\n",
      "   macro avg       0.84      0.97      0.89       805\n",
      "weighted avg       0.97      0.96      0.96       805\n",
      "\n",
      "[[695  35]\n",
      " [  1  74]]\n",
      "=============en - pt=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.37      0.53       846\n",
      "           1       0.29      0.93      0.44       235\n",
      "\n",
      "    accuracy                           0.49      1081\n",
      "   macro avg       0.62      0.65      0.48      1081\n",
      "weighted avg       0.80      0.49      0.51      1081\n",
      "\n",
      "[[310 536]\n",
      " [ 17 218]]\n",
      "=============en - fr=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.94       602\n",
      "           1       0.56      0.77      0.65        79\n",
      "\n",
      "    accuracy                           0.90       681\n",
      "   macro avg       0.76      0.85      0.80       681\n",
      "weighted avg       0.92      0.90      0.91       681\n",
      "\n",
      "[[554  48]\n",
      " [ 18  61]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 7 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model no balancing strategy\n",
    "\n",
    "ouput_creator('en', model,d_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============es - en=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.90      0.81      4615\n",
      "           1       0.80      0.57      0.66      3298\n",
      "\n",
      "    accuracy                           0.76      7913\n",
      "   macro avg       0.77      0.73      0.74      7913\n",
      "weighted avg       0.77      0.76      0.75      7913\n",
      "\n",
      "[[4146  469]\n",
      " [1422 1876]]\n",
      "=============es - es=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       129\n",
      "           1       1.00      0.71      0.83        17\n",
      "\n",
      "    accuracy                           0.97       146\n",
      "   macro avg       0.98      0.85      0.90       146\n",
      "weighted avg       0.97      0.97      0.96       146\n",
      "\n",
      "[[129   0]\n",
      " [  5  12]]\n",
      "=============es - it=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      1076\n",
      "           1       0.23      0.48      0.31        42\n",
      "\n",
      "    accuracy                           0.92      1118\n",
      "   macro avg       0.60      0.71      0.63      1118\n",
      "weighted avg       0.95      0.92      0.93      1118\n",
      "\n",
      "[[1008   68]\n",
      " [  22   20]]\n",
      "=============es - hi=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.87      0.92       730\n",
      "           1       0.41      0.88      0.56        75\n",
      "\n",
      "    accuracy                           0.87       805\n",
      "   macro avg       0.70      0.87      0.74       805\n",
      "weighted avg       0.93      0.87      0.89       805\n",
      "\n",
      "[[634  96]\n",
      " [  9  66]]\n",
      "=============es - pt=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95       846\n",
      "           1       0.82      0.77      0.80       235\n",
      "\n",
      "    accuracy                           0.91      1081\n",
      "   macro avg       0.88      0.86      0.87      1081\n",
      "weighted avg       0.91      0.91      0.91      1081\n",
      "\n",
      "[[807  39]\n",
      " [ 53 182]]\n",
      "=============es - fr=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       602\n",
      "           1       0.75      0.62      0.68        79\n",
      "\n",
      "    accuracy                           0.93       681\n",
      "   macro avg       0.85      0.80      0.82       681\n",
      "weighted avg       0.93      0.93      0.93       681\n",
      "\n",
      "[[586  16]\n",
      " [ 30  49]]\n"
     ]
    }
   ],
   "source": [
    "ouput_creator('es', model,d_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============it - en=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.43      0.55      4615\n",
      "           1       0.50      0.80      0.61      3298\n",
      "\n",
      "    accuracy                           0.58      7913\n",
      "   macro avg       0.62      0.61      0.58      7913\n",
      "weighted avg       0.64      0.58      0.57      7913\n",
      "\n",
      "[[1984 2631]\n",
      " [ 674 2624]]\n",
      "=============it - es=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.93      1291\n",
      "           1       0.49      0.59      0.54       168\n",
      "\n",
      "    accuracy                           0.88      1459\n",
      "   macro avg       0.72      0.76      0.74      1459\n",
      "weighted avg       0.89      0.88      0.89      1459\n",
      "\n",
      "[[1190  101]\n",
      " [  69   99]]\n",
      "=============it - it=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       108\n",
      "           1       0.75      0.75      0.75         4\n",
      "\n",
      "    accuracy                           0.98       112\n",
      "   macro avg       0.87      0.87      0.87       112\n",
      "weighted avg       0.98      0.98      0.98       112\n",
      "\n",
      "[[107   1]\n",
      " [  1   3]]\n",
      "=============it - hi=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.27      0.42       730\n",
      "           1       0.11      0.89      0.20        75\n",
      "\n",
      "    accuracy                           0.32       805\n",
      "   macro avg       0.54      0.58      0.31       805\n",
      "weighted avg       0.88      0.32      0.40       805\n",
      "\n",
      "[[194 536]\n",
      " [  8  67]]\n",
      "=============it - pt=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95       846\n",
      "           1       0.88      0.77      0.82       235\n",
      "\n",
      "    accuracy                           0.93      1081\n",
      "   macro avg       0.91      0.87      0.89      1081\n",
      "weighted avg       0.93      0.93      0.93      1081\n",
      "\n",
      "[[820  26]\n",
      " [ 53 182]]\n",
      "=============it - fr=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       602\n",
      "           1       0.72      0.73      0.73        79\n",
      "\n",
      "    accuracy                           0.94       681\n",
      "   macro avg       0.84      0.85      0.84       681\n",
      "weighted avg       0.94      0.94      0.94       681\n",
      "\n",
      "[[579  23]\n",
      " [ 21  58]]\n"
     ]
    }
   ],
   "source": [
    "ouput_creator('it', model,d_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============hi - en=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.73      0.78      4615\n",
      "           1       0.68      0.80      0.73      3298\n",
      "\n",
      "    accuracy                           0.76      7913\n",
      "   macro avg       0.76      0.76      0.76      7913\n",
      "weighted avg       0.77      0.76      0.76      7913\n",
      "\n",
      "[[3368 1247]\n",
      " [ 663 2635]]\n",
      "=============hi - es=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.80      0.88      1291\n",
      "           1       0.34      0.80      0.48       168\n",
      "\n",
      "    accuracy                           0.80      1459\n",
      "   macro avg       0.65      0.80      0.68      1459\n",
      "weighted avg       0.90      0.80      0.83      1459\n",
      "\n",
      "[[1030  261]\n",
      " [  33  135]]\n",
      "=============hi - it=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.50      0.67      1076\n",
      "           1       0.06      0.88      0.12        42\n",
      "\n",
      "    accuracy                           0.52      1118\n",
      "   macro avg       0.53      0.69      0.39      1118\n",
      "weighted avg       0.96      0.52      0.65      1118\n",
      "\n",
      "[[543 533]\n",
      " [  5  37]]\n",
      "=============hi - hi=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        73\n",
      "           1       0.89      1.00      0.94         8\n",
      "\n",
      "    accuracy                           0.99        81\n",
      "   macro avg       0.94      0.99      0.97        81\n",
      "weighted avg       0.99      0.99      0.99        81\n",
      "\n",
      "[[72  1]\n",
      " [ 0  8]]\n",
      "=============hi - pt=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.51      0.66       846\n",
      "           1       0.32      0.83      0.47       235\n",
      "\n",
      "    accuracy                           0.58      1081\n",
      "   macro avg       0.62      0.67      0.56      1081\n",
      "weighted avg       0.79      0.58      0.62      1081\n",
      "\n",
      "[[434 412]\n",
      " [ 39 196]]\n",
      "=============hi - fr=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.67      0.79       602\n",
      "           1       0.24      0.78      0.37        79\n",
      "\n",
      "    accuracy                           0.69       681\n",
      "   macro avg       0.60      0.73      0.58       681\n",
      "weighted avg       0.88      0.69      0.74       681\n",
      "\n",
      "[[406 196]\n",
      " [ 17  62]]\n"
     ]
    }
   ],
   "source": [
    "ouput_creator('hi', model,d_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============pt - en=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.60      0.68      4615\n",
      "           1       0.58      0.78      0.66      3298\n",
      "\n",
      "    accuracy                           0.67      7913\n",
      "   macro avg       0.68      0.69      0.67      7913\n",
      "weighted avg       0.70      0.67      0.67      7913\n",
      "\n",
      "[[2768 1847]\n",
      " [ 742 2556]]\n",
      "=============pt - es=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91      1291\n",
      "           1       0.40      0.74      0.52       168\n",
      "\n",
      "    accuracy                           0.84      1459\n",
      "   macro avg       0.68      0.80      0.71      1459\n",
      "weighted avg       0.90      0.84      0.86      1459\n",
      "\n",
      "[[1108  183]\n",
      " [  44  124]]\n",
      "=============pt - it=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.91      1076\n",
      "           1       0.13      0.57      0.22        42\n",
      "\n",
      "    accuracy                           0.85      1118\n",
      "   macro avg       0.56      0.71      0.57      1118\n",
      "weighted avg       0.95      0.85      0.89      1118\n",
      "\n",
      "[[921 155]\n",
      " [ 18  24]]\n",
      "=============pt - hi=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.68      0.81       730\n",
      "           1       0.23      0.92      0.37        75\n",
      "\n",
      "    accuracy                           0.70       805\n",
      "   macro avg       0.61      0.80      0.59       805\n",
      "weighted avg       0.92      0.70      0.76       805\n",
      "\n",
      "[[496 234]\n",
      " [  6  69]]\n",
      "=============pt - pt=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        85\n",
      "           1       1.00      0.88      0.93        24\n",
      "\n",
      "    accuracy                           0.97       109\n",
      "   macro avg       0.98      0.94      0.96       109\n",
      "weighted avg       0.97      0.97      0.97       109\n",
      "\n",
      "[[85  0]\n",
      " [ 3 21]]\n",
      "=============pt - fr=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.73      0.83       602\n",
      "           1       0.26      0.72      0.38        79\n",
      "\n",
      "    accuracy                           0.73       681\n",
      "   macro avg       0.61      0.73      0.60       681\n",
      "weighted avg       0.87      0.73      0.78       681\n",
      "\n",
      "[[440 162]\n",
      " [ 22  57]]\n"
     ]
    }
   ],
   "source": [
    "ouput_creator('pt', model,d_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============fr - en=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70      4615\n",
      "           1       0.58      0.58      0.58      3298\n",
      "\n",
      "    accuracy                           0.65      7913\n",
      "   macro avg       0.64      0.64      0.64      7913\n",
      "weighted avg       0.65      0.65      0.65      7913\n",
      "\n",
      "[[3223 1392]\n",
      " [1378 1920]]\n",
      "=============fr - es=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1291\n",
      "           1       0.85      0.57      0.68       168\n",
      "\n",
      "    accuracy                           0.94      1459\n",
      "   macro avg       0.90      0.78      0.82      1459\n",
      "weighted avg       0.93      0.94      0.93      1459\n",
      "\n",
      "[[1274   17]\n",
      " [  73   95]]\n",
      "=============fr - it=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96      1076\n",
      "           1       0.28      0.69      0.40        42\n",
      "\n",
      "    accuracy                           0.92      1118\n",
      "   macro avg       0.63      0.81      0.68      1118\n",
      "weighted avg       0.96      0.92      0.94      1118\n",
      "\n",
      "[[1001   75]\n",
      " [  13   29]]\n",
      "=============fr - hi=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.65      0.79       730\n",
      "           1       0.22      0.96      0.36        75\n",
      "\n",
      "    accuracy                           0.68       805\n",
      "   macro avg       0.61      0.81      0.58       805\n",
      "weighted avg       0.92      0.68      0.75       805\n",
      "\n",
      "[[478 252]\n",
      " [  3  72]]\n",
      "=============fr - pt=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       846\n",
      "           1       0.81      0.81      0.81       235\n",
      "\n",
      "    accuracy                           0.92      1081\n",
      "   macro avg       0.88      0.88      0.88      1081\n",
      "weighted avg       0.92      0.92      0.92      1081\n",
      "\n",
      "[[800  46]\n",
      " [ 45 190]]\n",
      "=============fr - fr=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        61\n",
      "           1       1.00      0.62      0.77         8\n",
      "\n",
      "    accuracy                           0.96        69\n",
      "   macro avg       0.98      0.81      0.87        69\n",
      "weighted avg       0.96      0.96      0.95        69\n",
      "\n",
      "[[61  0]\n",
      " [ 3  5]]\n"
     ]
    }
   ],
   "source": [
    "ouput_creator('fr', model,d_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
