{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "torch.manual_seed(666)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "# pip install seaborn\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core import Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"Dataset.auto_read_files\" is deprecated after version 1.0.69. Please use \"Dataset.Tabular.from_delimited_files\" instead. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n",
      "DatasetDefinition class is deprecated after version 1.0.69. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n",
      "The constructor of Dataset is deprecated after version 1.0.69. Please use factory methods from \"Dataset.Tabular\" and \"Dataset.File\" to create dataset instances. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n",
      "\"Dataset.get\" is deprecated after version 1.0.69. Please use \"Dataset.get_by_name\" and \"Dataset.get_by_id\" to retrieve dataset. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7794\n"
     ]
    }
   ],
   "source": [
    "# get the configs for the Azure Workspace\n",
    "ws = Workspace.from_config()\n",
    "# initialize location of the data (blob)\n",
    "datastore_name = 'mlthesisdatablob'\n",
    "dataset_name='data.cleaned.raw'\n",
    "#load the data of english examples\n",
    "datastore = Datastore.get(ws, datastore_name)\n",
    "datapath = datastore.path('data_all_lang_downsampling.csv')\n",
    "dataset = Dataset.auto_read_files(datapath)\n",
    "\n",
    "d = dataset.register(workspace=ws, name=dataset_name, exist_ok=True, update_if_exist=True)\n",
    "\n",
    "dataset = Dataset.get(ws,dataset_name)\n",
    "# create a dataframe\n",
    "df = dataset.to_pandas_dataframe()\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "#Alternatively: Load data from csv in directory \n",
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('data_all_lang_downsampling.csv')\n",
    "\n",
    "#df.head()\n",
    "## Preprocessing \n",
    "\n",
    "# create a dataset for each language\n",
    "\n",
    "df_en = df[df.lang_final=='en']\n",
    "df_es = df[df.lang_final=='es']\n",
    "df_it = df[df.lang_final=='it']\n",
    "df_hi = df[df.lang_final=='hi']\n",
    "df_pt = df[df.lang_final=='pt']\n",
    "df_fr = df[df.lang_final=='fr']\n",
    "\n",
    "# reset the indicies\n",
    "\n",
    "df_en.reset_index(drop=True, inplace=True)\n",
    "df_es.reset_index(drop=True, inplace=True)\n",
    "df_it.reset_index(drop=True, inplace=True)\n",
    "df_hi.reset_index(drop=True, inplace=True)\n",
    "df_pt.reset_index(drop=True, inplace=True)\n",
    "df_fr.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split_df(df, test_size , val_size, random_state, stratify_column):\n",
    "    # create a intermediary df and the test set\n",
    "    val_s = val_size/(1-test_size)\n",
    "    df_int, df_test = train_test_split(df,\n",
    "                                       stratify=df[stratify_column],\n",
    "                                       test_size=test_size,\n",
    "                                       random_state=random_state)\n",
    "    df_train, df_val = train_test_split(df_int,\n",
    "                                       stratify=df_int[stratify_column],\n",
    "                                       test_size=test_size,\n",
    "                                       random_state=random_state)\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_test = df_test.reset_index(drop=True)\n",
    "    df_val = df_val.reset_index(drop=True)\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "\n",
    "MODEL_TYPE = 'xlm-roberta-base'\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)\n",
    "\n",
    "#from transformers import XLMRobertaForSequenceClassification\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
    "    MODEL_TYPE, \n",
    "    num_labels = 2, # The number of output labels. 2 for binary classification.\n",
    ")\n",
    "\n",
    "# Send the model to the device.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The code for the following cell was created using inspiration from : \n",
    "https://www.kaggle.com/vbookshelf/basics-of-bert-and-xlm-roberta-pytorch\"\"\"\n",
    "class CompDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df_data = df\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text = self.df_data.loc[index, 'text_feat_clean']\n",
    "        \n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(text,\n",
    "                                             add_special_tokens = True,\n",
    "                                             max_length = 512,\n",
    "                                             truncation=True,\n",
    "                                             pad_to_max_length=True,\n",
    "                                             return_attention_mask=True,\n",
    "                                             return_tensors ='pt')\n",
    "        \n",
    "        # These are torch tensors already.\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "        \n",
    "        # Convert the target to a torch tensor\n",
    "        target = torch.tensor(self.df_data.loc[index, 'target'])\n",
    "\n",
    "        sample = (padded_token_list, att_mask, target)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, optimizer, NUM_EPOCHS, model_name):    \n",
    "\n",
    "    \"\"\"The code for the following cell was created using inspiration from : \n",
    "    https://www.kaggle.com/vbookshelf/basics-of-bert-and-xlm-roberta-pytorch\"\"\"\n",
    "\n",
    "    # Set the seed.\n",
    "    seed_val = 101\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Store the average loss after each epoch so we can plot them.\n",
    "    loss_values = []\n",
    "\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch in range(0, NUM_EPOCHS):\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, NUM_EPOCHS))\n",
    "\n",
    "\n",
    "        stacked_val_labels = []\n",
    "        targets_list = []\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        print('Training...')\n",
    "\n",
    "        # put the model into train mode\n",
    "        model.train()\n",
    "\n",
    "        # This turns gradient calculations on and off.\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "            train_status = 'Batch ' + str(i) + ' of ' + str(len(train_dataloader))\n",
    "\n",
    "            print(train_status, end='\\r')\n",
    "\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                        attention_mask=b_input_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "            # Get the loss from the outputs tuple: (loss, logits)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Convert the loss from a torch tensor to a number.\n",
    "            # Calculate the total loss.\n",
    "            total_train_loss = total_train_loss + loss.item()\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # Use the optimizer to update the weights.\n",
    "            optimizer.step() \n",
    "\n",
    "\n",
    "\n",
    "        print('Train loss:' ,total_train_loss/len(train_dataloader))\n",
    "\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "\n",
    "        print('\\nValidation...')\n",
    "\n",
    "        # Put the model in evaluation mode.\n",
    "        model.eval()\n",
    "\n",
    "        # Turn off the gradient calculations.\n",
    "        # This tells the model not to compute or store gradients.\n",
    "        # This step saves memory and speeds up validation.\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_val_loss = 0\n",
    "\n",
    "\n",
    "        for j, batch in enumerate(val_dataloader):\n",
    "\n",
    "            val_status = 'Batch ' + str(j) + ' of ' + str(len(val_dataloader))\n",
    "\n",
    "            print(val_status, end='\\r')\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)      \n",
    "\n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "            # Get the loss from the outputs tuple: (loss, logits)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Convert the loss from a torch tensor to a number.\n",
    "            # Calculate the total loss.\n",
    "            total_val_loss = total_val_loss + loss.item()\n",
    "\n",
    "\n",
    "            # Get the preds\n",
    "            preds = outputs[1]\n",
    "\n",
    "\n",
    "            # Move preds to the CPU\n",
    "            val_preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            # Move the labels to the cpu\n",
    "            targets_np = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Append the labels to a numpy list\n",
    "            targets_list.extend(targets_np)\n",
    "\n",
    "            if j == 0:  # first batch\n",
    "                stacked_val_preds = val_preds\n",
    "\n",
    "            else:\n",
    "                stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n",
    "\n",
    "\n",
    "        # Calculate the validation accuracy\n",
    "        y_true = targets_list\n",
    "        y_pred = np.argmax(stacked_val_preds, axis=1)\n",
    "\n",
    "        val_acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "        print('Val loss:' ,total_val_loss/len(val_dataloader))\n",
    "        print('Val acc: ', val_acc)\n",
    "\n",
    "\n",
    "        # Save the Model\n",
    "        torch.save(model.state_dict(), model_name+'.pt')\n",
    "\n",
    "    return model, loss_values\n",
    "\n",
    "\n",
    "def evaluate(test_dataloader, model):    \n",
    "    targets_list = []\n",
    "    for j, batch in enumerate(test_dataloader):\n",
    "\n",
    "            inference_status = 'Batch ' + str(j+1) + ' of ' + str(len(test_dataloader))\n",
    "\n",
    "            print(inference_status, end='\\r')\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, \n",
    "                    attention_mask=b_input_mask)\n",
    "\n",
    "\n",
    "            # Get the preds\n",
    "            preds = outputs[0]\n",
    "\n",
    "\n",
    "            # Move preds to the CPU\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            # Move the labels to the cpu\n",
    "            targets_np = b_labels.to('cpu').numpy()\n",
    "\n",
    "            # Append the labels to a numpy list\n",
    "            targets_list.extend(targets_np)\n",
    "\n",
    "            # Stack the predictions.\n",
    "\n",
    "            if j == 0:  # first batch\n",
    "                stacked_preds = preds\n",
    "\n",
    "            else:\n",
    "                stacked_preds = np.vstack((stacked_preds, preds))\n",
    "            \n",
    "    y_true = targets_list\n",
    "    y_pred = np.argmax(stacked_preds, axis=1)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "            \n",
    "\n",
    "def x_language_eval(dataloader, model, fig_name):\n",
    "    y_true, y_pred = evaluate(dataloader, model)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    #figure = sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, cmap='Blues')\n",
    "    #fig = figure.get_figure()\n",
    "    #fig.savefig(fig_name)\n",
    "    \n",
    "def ouput_creator(train_lang, model, d_dataloader):\n",
    "    \n",
    "    for i in d_dataloader:\n",
    "        if i != train_lang:\n",
    "            print('============='+train_lang +' - '+i+'=============')\n",
    "            x_language_eval(d_dataloader[i], model, train_lang+'_'+i+'_b.png')\n",
    "        else:\n",
    "            print('============='+train_lang +' - '+i+'=============')\n",
    "            x_language_eval(test_dataloader, model, i+'_'+i+'_b.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_RATE = 2e-5\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "NUM_CORES = 6\n",
    "\n",
    "DF = df_fr\n",
    "\n",
    "MODEL_NAME = 'model_fr_downsampling'\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "              lr = L_RATE, \n",
    "              eps = 1e-8 \n",
    "            )\n",
    "\n",
    "\n",
    "df_train, df_val, df_test = train_val_test_split_df(DF, 0.1,0.2,565, 'target')\n",
    "\n",
    "train_data = CompDataset(df_train)\n",
    "val_data = CompDataset(df_val)\n",
    "test_data = CompDataset(df_test)\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        sampler = RandomSampler(train_data),\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        sampler = SequentialSampler(val_data),\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              sampler=SequentialSampler(test_data), \n",
    "                                              num_workers=NUM_CORES)\n",
    "\n",
    "\n",
    "english_dataloader = torch.utils.data.DataLoader(CompDataset(df_en),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "\n",
    "\n",
    "spanish_dataloader = torch.utils.data.DataLoader(CompDataset(df_es),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "italian_dataloader = torch.utils.data.DataLoader(CompDataset(df_it),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "hindi_dataloader = torch.utils.data.DataLoader(CompDataset(df_hi),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "portuguese_dataloader = torch.utils.data.DataLoader(CompDataset(df_pt),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "french_dataloader = torch.utils.data.DataLoader(CompDataset(df_fr),\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True,\n",
    "                                       num_workers=NUM_CORES)\n",
    "\n",
    "d_dataloader = {'en':english_dataloader,\n",
    "                'es':spanish_dataloader,\n",
    "                'it':italian_dataloader,\n",
    "                'hi':hindi_dataloader,\n",
    "                'pt':portuguese_dataloader, \n",
    "                'fr':french_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "Train loss: 0.6986815184354782\n",
      "\n",
      "Validation...\n",
      "Val loss: 0.6793171167373657\n",
      "Val acc:  0.5333333333333333\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "Train loss: 0.5634130742400885\n",
      "\n",
      "Validation...\n",
      "Val loss: 0.4686014652252197\n",
      "Val acc:  0.9333333333333333\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "Train loss: 0.2650660932995379\n",
      "\n",
      "Validation...\n",
      "Val loss: 0.06836574524641037\n",
      "Val acc:  1.0\n"
     ]
    }
   ],
   "source": [
    "model, loss_stats = train(train_dataloader, optimizer, NUM_EPOCHS, MODEL_NAME)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============en - en=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.93       330\n",
      "           1       0.99      0.86      0.92       330\n",
      "\n",
      "    accuracy                           0.93       660\n",
      "   macro avg       0.93      0.93      0.93       660\n",
      "weighted avg       0.93      0.93      0.93       660\n",
      "\n",
      "[[326   4]\n",
      " [ 45 285]]\n",
      "=============en - es=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.87       168\n",
      "           1       0.91      0.81      0.86       168\n",
      "\n",
      "    accuracy                           0.87       336\n",
      "   macro avg       0.87      0.87      0.87       336\n",
      "weighted avg       0.87      0.87      0.87       336\n",
      "\n",
      "[[155  13]\n",
      " [ 32 136]]\n",
      "=============en - it=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84        42\n",
      "           1       0.89      0.76      0.82        42\n",
      "\n",
      "    accuracy                           0.83        84\n",
      "   macro avg       0.84      0.83      0.83        84\n",
      "weighted avg       0.84      0.83      0.83        84\n",
      "\n",
      "[[38  4]\n",
      " [10 32]]\n",
      "=============en - hi=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        75\n",
      "           1       0.97      1.00      0.99        75\n",
      "\n",
      "    accuracy                           0.99       150\n",
      "   macro avg       0.99      0.99      0.99       150\n",
      "weighted avg       0.99      0.99      0.99       150\n",
      "\n",
      "[[73  2]\n",
      " [ 0 75]]\n",
      "=============en - pt=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.76      0.81       235\n",
      "           1       0.78      0.88      0.83       235\n",
      "\n",
      "    accuracy                           0.82       470\n",
      "   macro avg       0.82      0.82      0.82       470\n",
      "weighted avg       0.82      0.82      0.82       470\n",
      "\n",
      "[[178  57]\n",
      " [ 28 207]]\n",
      "=============en - fr=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.99      0.83        79\n",
      "           1       0.98      0.62      0.76        79\n",
      "\n",
      "    accuracy                           0.80       158\n",
      "   macro avg       0.85      0.80      0.80       158\n",
      "weighted avg       0.85      0.80      0.80       158\n",
      "\n",
      "[[78  1]\n",
      " [30 49]]\n"
     ]
    }
   ],
   "source": [
    "# model downsampling\n",
    "\n",
    "ouput_creator('en', model,d_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============es - en=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.49      0.64      3298\n",
      "           1       0.65      0.95      0.77      3298\n",
      "\n",
      "    accuracy                           0.72      6596\n",
      "   macro avg       0.78      0.72      0.70      6596\n",
      "weighted avg       0.78      0.72      0.70      6596\n",
      "\n",
      "[[1609 1689]\n",
      " [ 157 3141]]\n",
      "=============es - es=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.94      0.86        17\n",
      "           1       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.85        34\n",
      "   macro avg       0.86      0.85      0.85        34\n",
      "weighted avg       0.86      0.85      0.85        34\n",
      "\n",
      "[[16  1]\n",
      " [ 4 13]]\n",
      "=============es - it=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.62      0.72        42\n",
      "           1       0.70      0.90      0.79        42\n",
      "\n",
      "    accuracy                           0.76        84\n",
      "   macro avg       0.79      0.76      0.76        84\n",
      "weighted avg       0.79      0.76      0.76        84\n",
      "\n",
      "[[26 16]\n",
      " [ 4 38]]\n",
      "=============es - hi=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.56      0.72        75\n",
      "           1       0.69      1.00      0.82        75\n",
      "\n",
      "    accuracy                           0.78       150\n",
      "   macro avg       0.85      0.78      0.77       150\n",
      "weighted avg       0.85      0.78      0.77       150\n",
      "\n",
      "[[42 33]\n",
      " [ 0 75]]\n",
      "=============es - pt=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.77      0.85       235\n",
      "           1       0.81      0.96      0.88       235\n",
      "\n",
      "    accuracy                           0.87       470\n",
      "   macro avg       0.88      0.87      0.86       470\n",
      "weighted avg       0.88      0.87      0.86       470\n",
      "\n",
      "[[181  54]\n",
      " [  9 226]]\n",
      "=============es - fr=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.75      0.81        79\n",
      "           1       0.78      0.91      0.84        79\n",
      "\n",
      "    accuracy                           0.83       158\n",
      "   macro avg       0.84      0.83      0.83       158\n",
      "weighted avg       0.84      0.83      0.83       158\n",
      "\n",
      "[[59 20]\n",
      " [ 7 72]]\n"
     ]
    }
   ],
   "source": [
    "ouput_creator('es', model,d_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============it - en=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67      3298\n",
      "           1       0.88      0.02      0.03      3298\n",
      "\n",
      "    accuracy                           0.51      6596\n",
      "   macro avg       0.69      0.51      0.35      6596\n",
      "weighted avg       0.69      0.51      0.35      6596\n",
      "\n",
      "[[3291    7]\n",
      " [3247   51]]\n",
      "=============it - es=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      1.00      0.67       168\n",
      "           1       1.00      0.03      0.06       168\n",
      "\n",
      "    accuracy                           0.51       336\n",
      "   macro avg       0.75      0.51      0.37       336\n",
      "weighted avg       0.75      0.51      0.37       336\n",
      "\n",
      "[[168   0]\n",
      " [163   5]]\n",
      "=============it - it=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73         5\n",
      "           1       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.67         9\n",
      "   macro avg       0.67      0.65      0.65         9\n",
      "weighted avg       0.67      0.67      0.66         9\n",
      "\n",
      "[[4 1]\n",
      " [2 2]]\n",
      "=============it - hi=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      1.00      0.68        75\n",
      "           1       1.00      0.05      0.10        75\n",
      "\n",
      "    accuracy                           0.53       150\n",
      "   macro avg       0.76      0.53      0.39       150\n",
      "weighted avg       0.76      0.53      0.39       150\n",
      "\n",
      "[[75  0]\n",
      " [71  4]]\n",
      "=============it - pt=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      1.00      0.67       235\n",
      "           1       1.00      0.03      0.07       235\n",
      "\n",
      "    accuracy                           0.52       470\n",
      "   macro avg       0.75      0.52      0.37       470\n",
      "weighted avg       0.75      0.52      0.37       470\n",
      "\n",
      "[[235   0]\n",
      " [227   8]]\n",
      "=============it - fr=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67        79\n",
      "           1       0.00      0.00      0.00        79\n",
      "\n",
      "    accuracy                           0.50       158\n",
      "   macro avg       0.25      0.50      0.33       158\n",
      "weighted avg       0.25      0.50      0.33       158\n",
      "\n",
      "[[79  0]\n",
      " [79  0]]\n"
     ]
    }
   ],
   "source": [
    "ouput_creator('it', model,d_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============hi - en=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.62      0.69      3298\n",
      "           1       0.69      0.83      0.75      3298\n",
      "\n",
      "    accuracy                           0.72      6596\n",
      "   macro avg       0.73      0.72      0.72      6596\n",
      "weighted avg       0.73      0.72      0.72      6596\n",
      "\n",
      "[[2047 1251]\n",
      " [ 572 2726]]\n",
      "=============hi - es=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.52      0.62       168\n",
      "           1       0.64      0.84      0.72       168\n",
      "\n",
      "    accuracy                           0.68       336\n",
      "   macro avg       0.70      0.68      0.67       336\n",
      "weighted avg       0.70      0.68      0.67       336\n",
      "\n",
      "[[ 88  80]\n",
      " [ 27 141]]\n",
      "=============hi - it=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.36      0.48        42\n",
      "           1       0.58      0.88      0.70        42\n",
      "\n",
      "    accuracy                           0.62        84\n",
      "   macro avg       0.66      0.62      0.59        84\n",
      "weighted avg       0.66      0.62      0.59        84\n",
      "\n",
      "[[15 27]\n",
      " [ 5 37]]\n",
      "=============hi - hi=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88         8\n",
      "           1       0.86      0.86      0.86         7\n",
      "\n",
      "    accuracy                           0.87        15\n",
      "   macro avg       0.87      0.87      0.87        15\n",
      "weighted avg       0.87      0.87      0.87        15\n",
      "\n",
      "[[7 1]\n",
      " [1 6]]\n",
      "=============hi - pt=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.24      0.36       235\n",
      "           1       0.54      0.91      0.68       235\n",
      "\n",
      "    accuracy                           0.57       470\n",
      "   macro avg       0.64      0.57      0.52       470\n",
      "weighted avg       0.64      0.57      0.52       470\n",
      "\n",
      "[[ 56 179]\n",
      " [ 21 214]]\n",
      "=============hi - fr=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.25      0.35        79\n",
      "           1       0.52      0.82      0.64        79\n",
      "\n",
      "    accuracy                           0.54       158\n",
      "   macro avg       0.56      0.54      0.50       158\n",
      "weighted avg       0.56      0.54      0.50       158\n",
      "\n",
      "[[20 59]\n",
      " [14 65]]\n"
     ]
    }
   ],
   "source": [
    "ouput_creator('hi', model,d_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============pt - en=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.27      0.39      3298\n",
      "           1       0.55      0.90      0.68      3298\n",
      "\n",
      "    accuracy                           0.58      6596\n",
      "   macro avg       0.64      0.58      0.54      6596\n",
      "weighted avg       0.64      0.58      0.54      6596\n",
      "\n",
      "[[ 884 2414]\n",
      " [ 337 2961]]\n",
      "=============pt - es=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.44      0.56       168\n",
      "           1       0.61      0.87      0.72       168\n",
      "\n",
      "    accuracy                           0.65       336\n",
      "   macro avg       0.69      0.65      0.64       336\n",
      "weighted avg       0.69      0.65      0.64       336\n",
      "\n",
      "[[ 74  94]\n",
      " [ 22 146]]\n",
      "=============pt - it=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.62      0.60        42\n",
      "           1       0.59      0.55      0.57        42\n",
      "\n",
      "    accuracy                           0.58        84\n",
      "   macro avg       0.58      0.58      0.58        84\n",
      "weighted avg       0.58      0.58      0.58        84\n",
      "\n",
      "[[26 16]\n",
      " [19 23]]\n",
      "=============pt - hi=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.09      0.16        75\n",
      "           1       0.51      0.95      0.66        75\n",
      "\n",
      "    accuracy                           0.52       150\n",
      "   macro avg       0.57      0.52      0.41       150\n",
      "weighted avg       0.57      0.52      0.41       150\n",
      "\n",
      "[[ 7 68]\n",
      " [ 4 71]]\n",
      "=============pt - pt=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        24\n",
      "           1       0.96      0.96      0.96        23\n",
      "\n",
      "    accuracy                           0.96        47\n",
      "   macro avg       0.96      0.96      0.96        47\n",
      "weighted avg       0.96      0.96      0.96        47\n",
      "\n",
      "[[23  1]\n",
      " [ 1 22]]\n",
      "=============pt - fr=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.47      0.56        79\n",
      "           1       0.60      0.81      0.69        79\n",
      "\n",
      "    accuracy                           0.64       158\n",
      "   macro avg       0.66      0.64      0.63       158\n",
      "weighted avg       0.66      0.64      0.63       158\n",
      "\n",
      "[[37 42]\n",
      " [15 64]]\n"
     ]
    }
   ],
   "source": [
    "ouput_creator('pt', model,d_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============fr - en=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.32      0.45      3298\n",
      "           1       0.57      0.90      0.70      3298\n",
      "\n",
      "    accuracy                           0.61      6596\n",
      "   macro avg       0.66      0.61      0.57      6596\n",
      "weighted avg       0.66      0.61      0.57      6596\n",
      "\n",
      "[[1068 2230]\n",
      " [ 346 2952]]\n",
      "=============fr - es=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.59      0.68       168\n",
      "           1       0.67      0.85      0.75       168\n",
      "\n",
      "    accuracy                           0.72       336\n",
      "   macro avg       0.73      0.72      0.71       336\n",
      "weighted avg       0.73      0.72      0.71       336\n",
      "\n",
      "[[ 99  69]\n",
      " [ 26 142]]\n",
      "=============fr - it=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.43      0.59        42\n",
      "           1       0.63      0.98      0.77        42\n",
      "\n",
      "    accuracy                           0.70        84\n",
      "   macro avg       0.79      0.70      0.68        84\n",
      "weighted avg       0.79      0.70      0.68        84\n",
      "\n",
      "[[18 24]\n",
      " [ 1 41]]\n",
      "=============fr - hi=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.03        75\n",
      "           1       0.50      1.00      0.67        75\n",
      "\n",
      "    accuracy                           0.51       150\n",
      "   macro avg       0.75      0.51      0.35       150\n",
      "weighted avg       0.75      0.51      0.35       150\n",
      "\n",
      "[[ 1 74]\n",
      " [ 0 75]]\n",
      "=============fr - pt=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.38      0.55       235\n",
      "           1       0.61      0.98      0.75       235\n",
      "\n",
      "    accuracy                           0.68       470\n",
      "   macro avg       0.78      0.68      0.65       470\n",
      "weighted avg       0.78      0.68      0.65       470\n",
      "\n",
      "[[ 90 145]\n",
      " [  5 230]]\n",
      "=============fr - fr=============\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88         8\n",
      "           1       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.88        16\n",
      "   macro avg       0.88      0.88      0.88        16\n",
      "weighted avg       0.88      0.88      0.88        16\n",
      "\n",
      "[[7 1]\n",
      " [1 7]]\n"
     ]
    }
   ],
   "source": [
    "ouput_creator('fr', model,d_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
